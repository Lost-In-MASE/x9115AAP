# DETECTION OF DUPLICATE BUG REPORT - A STUDY

Aaditya Sriram (asriram4), Abhishek Ravi (aravi5), Parth Satra (pasatra)


## 1. ABSTRACT

Mining software repositories is an interdisciplinary domain, which focuses on using data mining technologies to deal with software engineering problems[1]. By leveraging data mining techniques, mining software repositories can uncover interesting information in software repositories and solve real-world software problems.

Testers or users submit bug reports to identify various issues with systems. Bug repositories are the central storage for the bug reports. Sometimes two or more bug reports correspond to the same defect and thus gives duplicate bug reports. To address the problem with duplicate bug reports, a person called a triager needs to manually label these bug reports as duplicates, and link them to their ”master” reports for subsequent maintenance work. However, in practice there are considerable duplicate bug reports sent daily; requesting triagers to manually label these bugs could be highly time consuming.

Several techniques that use similarity based metrics to detect candidate duplicate bug reports for manual verification, have been proposed to address the issue of manual bug triaging. Automating triaging has been proved challenging as two reports of the same bug could be written in various ways. There is still much room for improvement in terms of accuracy of duplicate detection process. In this paper we discuss the recent advances in the field of automated duplicate bug detection, findings and the scope for future work.


## 2. INTRODUCTION
In modern software development, software repositories are large-scale databases for storing the output of software development, e.g., source code, bugs, emails, and specifications. Due to complexities of software systems, software bugs are prevalent. A bug repository is a software repository, for storing details of bugs. It is important in managing software bugs. Software bugs are inevitable and fixing bugs is expensive in software development. Software companies spend over 45 percent of cost in fixing bugs [2]. To support information collection and to assist developers to handle bugs, large software projects deploy bug repositories, also called bug tracking systems. For many software projects, bug tracking systems play a central role in supporting collaboration between the developers and the users of the software [3]. In a bug repository, a bug is maintained as a bug report. A bug report contains textual description of the issue and the method of reproducing the bug. It is a structured record consisting of many fields. Commonly, they include summary, description, project, submitter, priority and so forth. The figure below shows a typical bug report. Each field carries a different type of information. For example, summary is a concise description of the defect problem while description is the detailed outline of what went wrong and how it happened. Both of them are in natural language format. Other fields such as project, priority try to characterize the defect from other perspectives. A status field in a bug is a representation of the different stages in a lifecycle of a bug. Due to complexities of software systems, software bugs are prevalent and inevitable. In a software project, its bug tracking system is usually accessible to testers and even to all end users. Once a bug manifests, people can submit a bug report depicting the detail of the bug.  Developers could triage, track, and comment on the various bugs that are reported. However multiple reports from different submitters may correspond to the same bug, which causes the problem of duplicate bug reports.

![Bug](https://github.com/Lost-In-MASE/x9115AAP/blob/master/paper/images/bug.png)

### 2.1 MOTIVATION
Bug reporting process is often uncoordinated and ad-hoc. It is very common that the same bugs could be reported more than once by different users. Hence, there is often a need for manual inspection to detect whether the bug has been reported before. If the incoming bug report is not reported before then the bug should be assigned to a developer. But if other users have reported the bug before then the bug should be classified as a "duplicate" and should be attached to the original first-reported “master” bug report. There are two challenges related to bug data that may affect the effective use of bug repositories in software development tasks and they are "large scale" and "low quality". On one hand, due to the daily-reported bugs, a large number of new bugs are stored in bug repositories. Taking an open source project, Eclipse, as an example, an average of 30 new bugs were reported to bug repositories per day in 2007 [7]; from 2001 to 2010, 333,371 bugs were reported to Eclipse by over 34,917 developers and users [8][9]. The process of classifying a bug report as a new bug and assigning to the right developer, or classifying as a duplicate of another existing bug and linking it to the master report, is referred to as triaging. Currently in the software development, this process is done manually i.e., there is a manual triager who has the required expertise to classify the incoming bugs. The Mozilla programmers reported in 2005 that “everyday, almost 300 bugs appear that need triaging. This is far too much for only the Mozilla programmers to handle” [10]. This is seems to be the most time consuming step of handling a bug and alleviating the burden of the triagers has gained a lot of importance over the past decade, in the field of automated software engineering. A lot of effort has been put in deploying an automated triaging system in the industry. Automated bug tracking has gained importance in the field of software engineering over the past few years because the of rapid pace with which the bugs are being reported each day. There are several challenges with detection of duplicate bugs. Since bug reporting distributed random process, reporting of duplicate bugs cannot be controlled at source. Moreover, bug reports are written in in natural-language text and hence the same issue can potentially be described in several ways. This makes it hard to identify duplicates. Vocabulary used by developers who report issue may be much different when compared to that of an end user. It would require an expert in the field to be a triager and detect duplicate bug reports. Thus recognizing duplicate reports is an important problem that, if solved, would enable developers to fix bugs faster, and prevent them from wasting time by addressing the same bug multiple times. Classifying bug reports as duplicates by applying some of the Data Mining techniques of “textual similarity” and “information retrieval” has been a very popular approach.


## 3. DATA
Most of the studies have used the bug repositories of open source projects - OpenOffice, Mozilla and Eclipse as their data for conducting their study. These repositories are very popular because these projects are all open source and have a large repository of bug reports and are freely accessible. These projects are also very different from one another in terms of purposes, users and implementation languages, and thus help in generalizing the conclusions of the experiments.


## 4. THE EVOLUTION
Every software system is prone to have bugs. Bugs are defects in the functionality of a software system. All bugs in the system cannot be found by developers and testers upfront. Some bugs always get passed to the end user. Hence we need a mechanism for the end users to pass on these bugs to the developers so that they can be fixed and a better version of the software can be provided for the end user. Thus, iteratively we have a new version for a software for users and in return users provide feedback in terms of bugs and improvements which become the requirement for the next version of the software. This complete process is called software development. As seen from the data provided in one of the papers, there were about 211843 bugs reported from October 2001 to October 2007 for Eclipse Project [5] and about 44249 in the year 2008, out of these only 16511 were unique bugs. With these many amounts of bugs getting reported for each project in a year, managing these bugs and identifying those that are unique is in itself quite challenging. Doing this completely manually meant dramatic loss of efficiency and time. It also reduced the speed of development. Thus, automatically identifying a possible smaller subset of these bugs which are identical would greatly help the triager. this was the main reason behind mining the software repositories and using various machine learning and data mining techniques to solve software engineering problems. 

It is important to know what a bug report contains in order to identify whether its duplicate and device a strategy to find one. Bug report normally contains a title, description about the bug, steps to reproduce the bug, name of the software and software version containing the bug, priority of the bug which is a measure of its impact on the software, logs from the software system when the bug occurred, exception trace from the system, person who reports the bug and patch report [6]. Some of this information may or may not be present in every bug report. Ideally the triager would consider all of this information in order to come to a conclusion whether the bug is a duplicate or not. But equality of any one single field in the bug report doesn’t necessarily mean that the two bugs under consideration are duplicates. 


### 4.1 Natural Language Processing for Duplicate Bug Elimination

Runeson et al. [13] perform one of the first studies on finding duplicate bug reports. They perform standard tokenization, stemming, and stop-word removal on the natural language text of bug reports. They characterize each bug report as a bag of word tokens and model as a feature vector, where each feature corresponds to a word in the bug report. The feature value in the vector is computed based on the following formula: 1 + log2(TF(word)); where TF is the term frequency of the word. Each bug report is then compared to other bug reports by computing the similarity of their corresponding feature vectors. Three similarity measures are analyzed: cosine, dice, and jaccard. Bug reports with high similarity scores are likely to be duplicates. Their study on bug reports of Sony Ericsson Mobile Communications show that cosine performs best and is able to detect 40% of the duplicate bug reports.


### 4.2 Natural Language Processing isn’t sufficient for accurate duplicate bug detection

Thus natural language processing became the most basic way to find duplicate bug reports by compare the title and the description of the two bugs under consideration to find out the similarities and if significant enough then pass it on to the triager for final review. Since description and title very narrative and can vary from person to person and language to language the accuracy was less and hence the process was less effective. As an improvement to this a paper in 2008 proposed that other elements such as exception trace should also be considered while identifying the duplicate bugs [6]. As suggested in the paper, title and description only provide the outer view of the software from the users perspective. Finding similarities just from the user front might not be sufficient to deduce that the bugs are identical. Two issues might look similar from outside but might be caused due to different reasons. Having said that it is also one of the first steps of detecting duplicate bugs. Thus, the authors have devised and new technique where they consider title and description as users perspective and also consider execution trace which is software’s internal reaction to the bug. The bug report always will not have the complete execution trace, hence the algorithm dynamically decides the weightage of each of these parameters and then calculates a similarity score based on these parameters. If the score is greater than a threshold then the similarity list is presented to the triager and corresponding duplicate bug is removed from the repository. Thus using both the internal software view as well as user’s perspective of the bug improves the efficiency and accuracy of bug detection. 


### 4.3 Duplicate bug reports, a different perspective.

Till this point duplicate bugs found were removed from the repositories to avoid unnecessary storage of duplicate bugs. However, researchers started asking if the duplicate bugs were actually bad or they also can add some value to the accuracy and help in bug solving [5]. It was found from a study that though bugs can be duplicates sometimes the duplicates also have some additional information regarding the defect or new feature request and this data can complement the original bug report to get a better understanding of the bug while solving it and also since bug report now has close to complete information, the bug detection henceforth will be more accurate and reliable. Thus the next phase in the bug detection was to be able to save the duplicate bugs efficiently and use the information to complement the original bug. Thus in paper [5] authors proposed an alternative to handle this problem. The original bugs are stored in the master repository and the duplicate bugs are stored in the extended repository which is then linked to each record in master repository. This way when a duplicate is found in the master repository, the extended repository linked with it is also checked for any additional information. If the new bug is found to be duplicate then it’s added to the extended repository else a new entry is added to the master repository. Though the results have shown better accuracy by considering both the perspectives, it might still be possible that instead of complementing the data from duplicate bug reports, it’s possible that the bug report has information that actually is misleading, which might badly affect the accuracy.


### 4.4 Use of N-Grams as features in Feature Vector

As described in previous sections and the other papers referenced prior to this, work has generally been focussed on being able to find duplicate bug reports based on multiple factors. One of the commonly used language features is checking if the language and text style is similar in multiple reports. Prior to the work done by Ashish Sureka and Pankaj Jalote [15] most of the work was built on word based comparison of text titles between bugs in the master repository and new bugs flowing into the system. In their work Ashish and Pankaj [15] work on devising a scalable approach to language model based duplicate bug detection, in particular they work on N-Grams based approach to be able to get better results and higher throughput with lesser false results (non duplicate reports being marked duplicates and duplicate reports not being caught, both count as errors). While word based approach give an overall idea of what is being spoken about, many languages including English have a lot of variations and exceptions when it comes to rules by which the language is used, the authors believe that using a character level approach which we refer to as N-Grams gives a better sense of the linguistic features being employed in the statement to put forth an idea. Such an approach would be able to find duplicates that are not very similar when it comes to the title text but the similarity can be captured in the overall sense that can be made out from the bug report data. A character based approach also lets you easily avoid having to reimplement the system for other languages, at character level the system is pretty independent of the language. This might sounds like a contradiction to what was mentioned earlier, but what we really mean is that linguistic features are better observed on a character level approach but overall the system itself is not language specific as we can avoid many processes that are done on a word level approach such as tokenization, stop-word removal and so on which are heavily language dependent.

For this process, the data for the experiment was from the well known bug report database of the eclipse development environment. This data is freely available for research purposes and overall it is known to be high quality data in terms of the range of data types and sets that one can obtain by simply downloading parts of the repository. The bug reports are also of high quality in terms of the meta-data associated with the reports, for example all bug reports are annotated from previous runs by triagers who marked them as duplicate or not. They provide ideal data that be used as part of the training phase and the validation phase of the duplicate bug detection system development.

Around 213k bug reports were extracted and processed to build the initial system that would eventually classify bugs as duplicate or not automatically. The study showed visible improvement over existing solution at the time of writing. The tool scored related bug reports based on similarity scores and then reported them which were verified initially so that one could get a better sense of how the system is performing. The different approaches tried in the experiment were title based comparison, title and description based comparison and also description and title (in that order) based scores. The results were interesting and suggested of more work to be done in that area for sure.

The experiment made some simplifications that could have hurt the overall effectiveness of the system, studying the effectiveness of a system on data from just one source could bias the system heavily for that particular case causing very good results but may perform poorly in different data cases. The overall generalization based on just eclipse data might not be the best thing to do especially in a scientific experiment such as this. A point that the authors portrayed as an improvement over other systems was the use of a larger dataset, this is akin to saying having 10 apples and accurately identifying the 10 as apples is different from getting 1000 apples and accurately identifying all 10000 as apples. A bigger dataset does not always translate to a more diverse dataset. A diverse dataset is an implicit testing tool for functional validity.

### 4.5 Use of SVM and Discriminative Model

There have been several studies on retrieving similar bug reports. However, the performance of these systems is quite low and is thus making it hard to apply them in practice. Sun et al. claim that the low performance can be attributed to the following limitations of the current methods [11]. First, all the techniques in [6] employ one or two features to describe the similarity between reports, despite the fact that other features are also available for effective measurement of similarity. Second, different features contribute differently towards determining similarities because when project contexts evolve, the relative importance of features may vary. This can cause the past techniques, which are largely based on absolute rating of importance, to deteriorate in their performance. More accurate results would mean more automation and less effort by triagers to find duplicate bug reports. To address this need, Sun et al. proposed a discriminative model based approach that further improves accuracy in retrieving duplicate bug reports by up to 43% on real bug report datasets in [11]. Different from the previous approaches that rank similar bug reports based on similarity score of vector space representation, they developed a discriminative model to retrieve similar bug reports from a bug repository. Sun et al. make use of some of the recent advances in information retrieval community that uses a classifier to retrieve similar documents from a collection. The model contrasts duplicate bug reports from non-duplicate bug reports and utilize this model to extract similar bug reports. The effectiveness of bug report retrieval system is strengthened by introducing many more relevant features to capture the similarity between bug reports. With the adoption of the discriminative model approach, the relative importance of each feature will be automatically determined by the model through assignment of an optimum weight. Thus, bug repository evolves along with the discriminative model to guarantee that the weights remain optimum at all time. In paper [11] the authors evaluate their discriminative model approach on three large bug report datasets from large programs including Firefox, an open source web browser, Eclipse, a popular open source integrated development environment, and OpenOffice, a well-known open source rich text editor. They begin their evaluation by investigating the applicability of the approach on different types of systems. In their study they showed that their approach results in  17–31%, 22–26%, and 35–43% improvement over state-of- the-art techniques [6] in OpenOffice, Firefox, and Eclipse datasets respectively using commonly available natural language information alone. 

![Bucket](https://github.com/Lost-In-MASE/x9115AAP/blob/master/paper/images/bucket.png)

Support Vector Machine (SVM) was the approach followed by Sun et al. to building a discriminative model or classifier based on a set of labeled vectors. In SVM, given a set of vectors, some belonging to a positive class and others belonging to a negative class. SVM tries to build a hyperplane that separates vectors belonging to the positive class from those of the negative class with the largest margin. All the reports in the repository are organized into a bucket structure. The bucket structure is a hash-map-like data structure. Each bucket contains a master report as the key and all the duplicates of the master as its value. Therefore, each bucket stands for a distinct defect, while all the reports in a bucket correspond to the same defect. Figure 2 shows the structure of the bucket diagrammatically. Figure 3 discriminative shows the overall framework of the approach. In general, there are three main steps in the system - preprocessing, training a discriminative model and retrieving duplicate bug reports. The first step, preprocessing, follows a standard natural language processing style of tokenization, stemming and stop-words removal. The second step, training a discriminative model, trains SVM classifier to answer the question “How likely are two bug reports duplicates of each other?”. The third step, retrieving duplicate bug reports, makes use of this classifier to retrieve relevant bug reports from the repository.

![SVM](https://github.com/Lost-In-MASE/x9115AAP/blob/master/paper/images/svm.png)

Figure 4 shows the experiment results of the seven runs on the three datasets. In the figure, the horizontal axis is the top N list size, and the vertical axis is the recall rate; J stands for [12], R stands for [13] and W is [6]. Suffix -1 or -2 represents weighing summaries 1 or 2. O corresponds to approach followed by [11]. From the three sub figures, we can easily come to the first conclusion that the technique [11] brings a lot of improvement. They were able to achieve 17–31% relative improvement in OpenOffice dataset, 22–26% in Firefox dataset and 35–43% in Eclipse dataset. They were able to conclude that manually empirically weighing summaries for traditional IR techniques can help improve limited performance as the six curves near the bottom of each sub figure are very close to one another.

![SVMResults](https://github.com/Lost-In-MASE/x9115AAP/blob/master/paper/images/svm_results.png)


### 4.6 Use of BM25F for Textual Similarity and REP for Retrieval

Sun et al. came up with another paper[14], an year after their previous paper[11]. This paper was an improvement over the previous paper which was based on detection of duplicate bugs based on discriminative models. One of the major drawbacks of the approach in [11] was the run time of the entire workflow. The major bottleneck was that they considered 54 different similarity features between the bug reports, to build the model. The runtime overhead was higher at the later part of the experiment as the number of bug report pairs in the training set is larger. Consequently, SVM needed more time to build a discriminative model. In the study [12], Sun et al. made the following contributions:

1) They proposed a new duplicate bug report retrieval model by extending the popular and effective similarity formula in information retrieval community, BM25F. They engineered the extension of BM25F to handle longer queries.
2) The improvised on the similarity process by not only considering the textual content of the bug reports but also other categorical information such as priority, product version and so on.
3) They analysed the applicability of duplicate bug report detection techniques on a total of more than 350,000 bug reports across bug repositories of various large open source programs including OpenOffice, Mozilla and Eclipse.
4) They improved the accuracy of state-of-the-art automated duplicate bug detection techniques [15] by 10-27% in recall rate@k (1 ≤ k ≤ 20) and 17-23% in mean average precision.
Figure below shows the workflow of the approach followed by Sun et al. in [14].


![BM25F](https://github.com/Lost-In-MASE/x9115AAP/blob/master/paper/images/bm25f.png)

### 4.7 Use of Contextual Information to improve accuracy of Duplicate Bug Detection

Many researchers have approached the bug-deduplication problem using off-the-shelf information-retrieval tools, such as BM25F [14]. In their work, Anahita et al., introduce a new approach [21] by extending the work done by Sun et al. in [14], to improve the accuracy of duplicate bug detection. They investigate the effect of contextual information - prior knowledge of software quality, software architecture and system development (LDA) topics. They demonstrate the effectiveness of the contextual bug-deduplication method on the bug repository of the Android ecosystem. Based on this experience, they conclude that researchers should not ignore the context of software engineering when using IR tools for deduplication. Anahita et al. exploit the domain knowledge about the software engineering process in general and about the system as well, to improve the bug report deduplication. Essentially, rather than naively and exclusively applying information-retrieval (IR) tools, they propose to take advantage of our knowledge of the software process and product. They hypothesize that bug reports are likely to refer to software qualities (non-functional requirements), or software functionalities (like architectural components). They utilize a few software dictionaries and word lists to extract the context implicit in each bug report. Finally, they compare the contextual word lists to the bug reports and record the comparison results as new features, in addition to the textual and categorical features of bug reports such as description, component, type, priority and many more, as proposed by Sun et al. in [14]. Figure below shows a typical table constructed for comparison, by including all the features.

![Table](https://github.com/Lost-In-MASE/x9115AAP/blob/master/paper/images/table.png)

In this research, they use five different contextual word lists to study the effect of various software engineering contexts on the accuracy of duplicate bug-report detection. These word lists include: Android architectural words [18], software Non-Functional Requirements words [19], Android topic words extracted applying Latent Dirichlet Allocation (LDA) method [20], Android topic words extracted applying Labeled-LDA method [20], and random English dictionary words. Random english dictionary words are used to measure the baseline results which are then used to show the improvement of using the words that carry contextual information. To retrieve the duplicate bug reports, several well-known machine-learning algorithms such as Logistic Regression, Naive Bayes, Decision Trees, K-Nearest Neighbors and Rule based classifiers are applied using Weka. To validate the retrieval approach Anahita et al. employed 10-fold cross validation. They show that their method results in 16.07% relative improvement in accuracy and an 87.59% relative improvement in Kappa measure (over the baseline). Kappa measure is an improvement in evaluation methodology when compared to Sun et al. [14] as it also considers true negatives duplicate cases.


### 4.8 Use of Software Engineering Domain Knowledge to improve accuracy of Duplicate Bug Detection

Anahita et al. first introduced the concept of using contextual words to improve the accuracy of bug de-duplication, in [21]. But they used Labelled LDA approach to extract contextual features. This procedure took 60 person-hours to create topic words. However, in the paper[22], the authors propose "software literature conext method", where the contextual words are extracted from a generic software-literature. This approach shows a significant improvement in time with a slight decrease in accuracy of prediction. The extraction of contextual words in the current approach took only half a person-hour. The general software-engineering features extracted from a textbook published in 2001, performed only marginally worse, showing the robustness of the software-literature context method.

Anahita et al. improved upon the work of Sun et al. [14] by adding contextual features using both labelled and unlabelled LDA generated word lists to the method used by Sun et al.. They reformulated the task as detecting duplicate bug reports and showed that the use of LDA produced strong improvements in accuracy, increasing by 16% over the results obtained by Sun et al.[14]. The results obtained by Aggarwal et al. in [22] are similar to the results reported by [21] but method requires far less time and effort and is more general and easy to share. The approach used by Aggarwal et al. is different from the approach described by Anahita et al in [21] as the former approach uses word lists extracted from software literature instead of using bug reports from the software projects to extract the word lists. These software-literature context word lists reflect the software development processes, and the evolution of project, and hence depict the bug report descriptions. The word lists used as contextual information were extracted from the following sources:
1)Pressman’s "Software Engineering: A Practitioner’s Approach" [23]
2) "The Busy Coder’s Guide to Android Development" by Murphy [24]
3) Eclipse platform documentation
4) OpenOffice Documentation
5) Mozilla Documentation
6) Random English words
English random words were used to determine that the specialized word lists were actually having a significant effect. The lists used were same as the ones used in [21]. Using these contextual word lists, the procedure proposed by Anahita et al. was followed on four bug datasets from open-source projects, Android, OpenOffice, Mozilla, and Eclipse (containing approximately 37, 000, 42, 000, 72, 000, and 29, 000 bug reports correspondingly). Similar to the previous paper, several well-known machine-learning algorithms such as Logistic Regression, Naive Bayes, Decision Trees, K-Nearest Neighbors and Rule based classifiers are applied using Weka to retrieve the duplicate bug reports and 10-fold cross validation was employed to validate the retrieval. Thus, in this study, Aggarwal et al., conclude that the software literature context method, by comparison, is much faster than the approach [22] and is marginally less accurate and has lesser kappa scores when compared to labelled LDA.


### 4.9 Use of Software Reduction techniques for effective Bug Triage

In the paper [9], Xuan et al. address the problem of data reduction for bug triage. They show how reduction of the bug data can save the labor cost of developers and improve the quality to facilitate the process of bug triage. In this paper,  Data reduction for bug triage aims to build a small-scale and high-quality set of bug data by removing bug reports and words, which are redundant or non-informative. They combine existing techniques of instance selection and feature selection to reduce the bug dimension and the word dimension. The reduced bug data contain fewer bug reports and fewer words than the original bug data without any loss of information i.e., they provide similar information over the original bug. The evaluation of the reduced bug data is done according to two criteria: the scale of a data set and the accuracy of bug triage. To avoid the bias of a single algorithm, we empirically examine the results of four instance selection algorithms and four feature selection algorithms.

Xuan et al. found that given an instance selection algorithm and a feature selection algorithm, the order of applying these two algorithms affects the results of bug triage. In this paper, Xuan et al. also propose a predictive model to determine the order of applying instance selection and feature selection. Such determination is referred to as prediction for reduction orders. They train a binary classifier on bug data sets with extracted attributes and predict the order of applying instance selection and feature selection for a new bug data set. In the experiments, Xual et al. evaluate the data reduction for bug triage on bug reports of two large open source projects, namely Eclipse and Mozilla. Experimental results show that applying the instance selection technique to the data set can reduce bug reports but the accuracy of bug triage may be decreased but by applying the feature selection technique the authors reduced words in the bug data and showed that the accuracy can be increased. They also showed that combining both techniques also increased the accuracy, as well as reduced bug reports and words.


## 5. RELATED WORK
This report shows research in duplicate bug detection through different phases of developments and improvements over time. Duplicate bug detection in itself is a very challenging problem and solution for which came from multiple disciplines such as Data Mining, Natural Language Processing, Information Retrieval, Textual Similarity, Machine Learning and many more. The development in this field has been greatly influenced by the developments in each of these fields. Hence it is important to also understand and track some of the developments in these fields which in future might empower developments in duplicate bug detection techniques with higher accuracy. 


### 5.1 Data Mining 

Data mining is the field of discovering similarities and differences in the data. Traditionally data mining started as a medium of knowledge discovery in database management systems[17]. Today we see wide applications of data mining to very very large volume, velocity and veracity of data termed as Big Data. There has been tremendous developments in this field, right from designings different algorithmic techniques like Regression, Decision Trees, Support Vector Machine, K-means, etc. to actually improving the performance of these algorithms for different volumes, velocity and veracity of the data. For example these algorithms work differently for streaming data like one from twitter and work differently on data from databases. Each of these techniques are designed to work in different situations, some work when data being predicted is categorical while some work  data being predicted is continuous. Apart from demonstrating that the use of contextual features improves bug-deduplication performance, they also investigate the effect of the number of added features on bug duplication.


### 5.2 Natural Language Processing

Natural Language processing is the study of interactions between human language and the computer. A typical form of natural language processing is Text mining. In text mining the system tries to understand the human language, translate it into quantitative measure and then summarize the properties to get a better insight into the data. This is the prime essential in today’s world where data is abundantly available in the form that is not structured, but is present in raw natural text. These include data from social media like twitter and facebook. This field has grown rapidly in summarizing and extracting patterns from natural language. Some of the examples include Twitter sentiment analyzer, duplicate bug detection, recommendation systems, Amazon Echo and so on. There has been research on tokenizing the language, finding correlation between words, associating keywords to a property like di to positivity and so on. These techniques are also useful in bug detection where the user enters the information related to the bug in natural language and the detection system must understand the details entered by the user find correlation between multiple other bugs which already exists to find duplicates. There has been lots of research on improving the accuracy and quality of correlation between words using techniques like principal component analysis. Improvement in this space directly affects the performance of duplicate bug detection and many other applications.


### 5.3 Textual Similarity

This topic is closely related to natural language processing. Natural language language can be various forms like English, Spanish, Hindi, etc. In each of these forms the bug can be reported and thus can have duplicates across multiple forms. Identifying the similarity between different or same forms is termed as textual similarity. It is important for the system to commonly understand all forms of textual natural language and still detect the duplicates. The paper [23] shows the example of Interpretability between English and Spanish. Research is also happening in improving the interpretability which eventually affects the accuracy. 


### 5.4 Information Retrieval
Information Retrieval (IR) is a popular topic in software engineering research due to the prevalence of natural language artifacts. In this section we cover relevant IR works and bug- deduplication studies.

Binkley et al. [26] applied a variety of IR techniques, such as latent semantic indexing (LSI) and Formal Concept Analysis (FCA) on different software repositories.  LSI is a generative probabilistic model for sets of discrete data proposed by Dumais et al. [27] while FCA is a mathematical theory of data analysis using formal contexts and concept lattices explained by Ganter, B. et al. [28]. They have addressed software problems like fault prediction, developer identification for a task, assisting engineers in understanding unfamiliar code, estimating the effort required to change a software system, and refactoring. Marcus et al. have used LSI to map the concepts expressed by the programmers (in queries) to the relevant parts in the source code [29]. Their method is built upon finding semantic similarities between the queries and modules of the software. Similarly, Poshyvanyk et al. have applied the FCA, LSI and LDA techniques in order to locate the concepts in the source code [30]and have used LDA to investigate conceptual coupling [31].Latent Dirichlet Allocation (LDA) is a generative model for documents in which each document is related to a group of topics is described by Blei et al. [32]. Blie et al. have demonstrated that a convexity-based variational approach for inference is a fast algorithm with reasonable performance. Hindle et al. have proposed and implemented a labeled topic extraction method based on labeling the extracted topics using non-functional requirement concepts [16]. Their approach is based on LDA topic extraction of non-functional requirements as they believe that non-functional requirements concepts apply across many software systems.

These are some of the key building blocks in the duplicate bug detection system, which highly impact the performance and accuracy of the duplicate bug detection system. These related topics are also constantly evolving with new techniques showing better results. 


## 6. RESULTS & CONCLUSION	

In this paper we have covered the evolution research in the field of Mining Software Repositories and Data Mining in Software Engineering, particularly with respect to the topic - Detection of Duplicate Bug Reports. We found that with each passing year, experts of the field came up with new approaches and techniques to improve the accuracy of duplicate bug detection. Getting high accuracy in this domain is a challenge as all the bugs are reported in natural language and even though the bugs are duplicate of each other, they could be reported easily. In 2007, Runeson et al. used the standard techniques associated with NLP for elimination of duplicate bugs. They were able to show that 40% of duplicate bugs could be detected. In 2008, Jalbert et al. also introduced a methodology that utilized common sequence matching for duplicate report detection. They were able to demonstrate a duplicate recall rate of above 70% for Firefox data set. Both these studies eliminated duplicate bugs. Sun et al. adopted the approach of reporting top-k similar bugs instead of elimination of duplicate bugs as Bettenburg et al. noticed that all duplicate bug reports are not necessarily bad and that one bug report might only provide a partial view of the defect, while multiple bug reports can complement one another [5]. 

As mentioned in 4.4, Sureka and Jalote propose an approach that consider not word tokens but n-grams as features in a feature vector that characterizes a bug report [15]. They show that their approach is able to detect duplicate bug reports with reasonable accuracy on a large bug report corpus from Eclipse. No comparative study however is performed to compare their approach with the other existing approaches. The experiments in the paper by Sun et al. [14], in 2011, show that BM25Fext improves recall rate by 3–13% and mean average precision (MAP) by 4–11% over normal BM25F. Sun et al. had also come up with a paper in 2010 [11] that made use of SVM to build a discriminative model for duplicate bug detection. They were able to achieve better results with this approach but the runtime of the experiment was very long. They improved their retrieval performance of REP and in comparison to their previous work based on SVM, REP increased recall rate by 10–27%, and MAP by 17–23%; compared to the work by Sureka and Jalote [15], REP performed with recall rate@k of 37–71% (1 ≤ k ≤ 20), and MAP of 46%. In 2013, Anahita et al, extended the work of Sun et al. and showed that their method of including contextual information results in 16.07% relative improvement in accuracy, over Sun et al. and an 87.59% relative improvement in Kappa measure over the baseline results obtained when using random english words [21]. In 2014, Aggarwal et al., extended the work of Anahita et al. [21] and Sun et al. [14] and used software engineering books and project documentation to obtain the contextual information. This method did not increase the accuracy of the detection but vastly improved the runtime when compared to  approach used in [21]. Xuan et al. showed that software bug report quality can be improved by reducing the feature dimension and word dimension by using data reduction techniques[9]. They showed that the accuracy of bug traiging, such as detection of duplicate bugs, can be improved if irrelevant and redundant data can be removed without losing valuable information.

In our opinion, the duplicate bug detection system has evolved tremendously. Being able to shortlist a few from thousands of bugs automatically which then the triager can mark as duplicates is incredible. Having said that, the system still has a vast scope of improvement. The process of duplicate bug detection is not completely automated as of now. There is still lots of manual intervention required. The bug reporting system itself is manual and contains a lot of text in natural language. System generated reports of a crash or a standardization of bug reporting techniques can improve the consistency in bug reporting. We see that system generated reports have become very common these days. Combination of such reports and the techniques discussed throughout the paper can yield a higher accuracy in detection of duplicate reports.


## 7. REFERENCES
[1] A. E. Hassan, “The road ahead for mining software repositories,” in Proc. Front. Softw. Maintenance, Sep. 2008, pp. 48–57.
[2] R. S. Pressman, Software Engineering: A Practitioner’s Approach, 7th ed. New York, NY, USA: McGraw-Hill, 2010.
[3] S. Breu, R. Premraj, J. Sillito, and T. Zimmermann, “Information needs in bug reports: Improving cooperation between developers and users,” in Proc. ACM Conf. Comput. Supported Cooperative Work, Feb. 2010, pp. 301–310.
[4] J. Sutherland. Business objects in corporate information systems. In ACM Computing Surveys, 2006. 
[5] Nicolas Bettenburg, Rahul Premraj, Thomas Zimmermann and Sunghun Kim; “Duplicate Bug Reports Considered Harmful ... Really?” in Software Maintenance, 2008. ICSM 2008
[6] Xiaoyin Wang, Lu Zhang, Tao Xie John Anvik, Jiasu Sun; "An Approach to Detecting Duplicate Bug Reports using Natural Language and Execution Information" in Software Engineering, ICSE 2008. ACM/IEEE 30th International Conference.
[7] J. Anvik and G. C. Murphy, “Reducing the effort of bug report triage: Recommenders for development-oriented decisions,” ACM Trans. Soft. Eng. Methodol., vol. 20, no. 3, article 10, Aug. 2011.
[8] J. Xuan, H. Jiang, Z. Ren, and W. Zou, “Developer prioritization in bug repositories,” in Proc. 34th Int. Conf. Softw. Eng., 2012, pp. 25–35
[9] Xuan, J., Jiang, H., Hu, Y., Ren, Z., Zou, W., Luo, Z., & Wu, X. (2015). Towards Effective Bug Triage with Software Data Reduction Techniques. Knowledge and Data Engineering, IEEE Transactions on, 27(1), 264-280.
[10] J. Anvik, L. Hiew, and G. C. Murphy. Coping with an open bug repository. In eclipse ’05: Proceedings of the 2005 OOPSLA workshop on Eclipse technology exchange, pages 35–39, 2005.
[11] C. Sun, D. Lo, X. Wang, J. Jiang, and S.-C. Khoo, “A discriminative model approach for accurate duplicate bug report retrieval,” in ICSE, 2010
[12] N. Jalbert and W. Weimer. Automated Duplicate Detection for Bug Tracking Systems. In proceedings of the International Conference on Dependable Systems and Networks, 2008.
[13] P. Runeson, M. Alexandersson, and O. Nyholm. Detection of Duplicate Defect Reports Using Natural Language Processing. In proceedings of the International Conference on Software Engineering, 2007.
[14] C. Sun, D. Lo, S.-C. Khoo, and J. Jiang, “Towards more accurate retrieval of duplicate bug reports,” in Proceedings of the IEEE/ACM International Conference on Automated Software Engineering, 2011.
[15] A. Sureka and P. Jalote, “Detecting duplicate bug report using character n-gram-based features,” in Proceedings of the 2010 Asia Pacific Software Engineering Conference, 2010, pp. 366–374.
[16]  A. Hindle, N. Ernst, M. W. Godfrey, R. C. Holt, and J. Mylopoulos, “Whats in a name? on the automated topic naming of software maintenance activities,” submission: http://softwareprocess.es/whats-in-a-name, vol. 125, pp. 150–155, 2010.
[17] https://en.wikipedia.org/wiki/Data_mining
[18] V. Guana, F. Rocha, A. Hindle, and E. Stroulia, “Do the stars align? multidimensional analysis of android’s layered architecture,” in Mining Software Repositories (MSR), 2012 9th IEEE Working Conference on. IEEE, 2012, pp. 124–127.
[19] A. Hindle, N. Ernst, M. Godfrey, and J. Mylopoulos, “Automated topic naming to support cross-project analysis of software maintenance activities,” in Proceedings of the 8th Working Conference on Mining Software Repositories. ACM, 2011, pp. 163–172.
[20] D. Han, C. Zhang, X. Fan, A. Hindle, K. Wong, and E. Stroulia, “Understanding android fragmentation with topic analysis of vendor- specific bugs.”
[21] Anahita Alipour , Abram Hindle , Eleni Stroulia, A contextual approach towards more accurate duplicate bug report detection, Proceedings of the 10th Working Conference on Mining Software Repositories, May 18-19, 2013, San Francisco, CA, USA
[22] Aggarwal, Karan, et al. "Detecting duplicate bug reports with software engineering domain knowledge." Software Analysis, Evolution and Reengineering (SANER), 2015 IEEE 22nd International Conference on. IEEE, 2015.
[23] http://www.aclweb.org/anthology/S15-2045
[24] R. S. Pressman and W. S. Jawadekar, “Software engineering,” New York 1992, 1987.
[25] M. L. Murphy, The Busy Coder’s Guide to Advanced Android Development. CommonsWare, LLC, 2009.
[26] D. Binkley and D. Lawrie, “Information retrieval applications in software maintenance and evolution,” Encyclopedia of Software Engineer- ing, 2009.
[27] S. Dumais, G. Furnas, T. Landauer, S. Deerwester, S. Deerwester et al., “Latent semantic indexing,” in Proceedings of the Text Retrieval Conference, 1995.
[28] B. Ganter, R. Wille, and R. Wille, Formal concept analysis. Springer Berlin, 1999.
[29] A. Marcus, A. Sergeyev, V. Rajlich, and J. I. Maletic, “An information retrieval approach to concept location in source code,” in Reverse Engineering, 2004. Proceedings. 11th Working Conference on. IEEE, 2004, pp. 214–223.
[30] D. Poshyvanyk and A. Marcus, “Combining formal concept analysis with information retrieval for concept location in source code,” in Program Comprehension, 2007. ICPC’07. 15th IEEE International Conference on. IEEE, 2007, pp. 37–48.
[31] D. Poshyvanyk, A. Marcus, R. Ferenc, and T. Gyimo ́thy, “Using information retrieval based coupling measures for impact analysis,” Empirical Software Engineering, vol. 14, no. 1, pp. 5–32, 2009.
[32] D. Blei, A. Ng, and M. Jordan, “Latent dirichlet allocation,” the Journal of machine Learning research, vol. 3, pp. 993–1022, 2003.


				
